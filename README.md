# TRF5 Scraper - Sistema de ExtraÃ§Ã£o de Processos Judiciais

Sistema profissional de web scraping desenvolvido com Scrapy para extrair dados de processos judiciais do Tribunal Regional Federal da 5Âª RegiÃ£o (TRF5). O sistema coleta informaÃ§Ãµes detalhadas de processos, incluindo dados principais, partes envolvidas e histÃ³rico de movimentaÃ§Ãµes, armazenando tudo de forma estruturada no MongoDB.

## ğŸ“‹ Ãndice

- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Requisitos](#requisitos)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
  - [InstalaÃ§Ã£o com Docker (Recomendado)](#instalaÃ§Ã£o-com-docker-recomendado)
  - [InstalaÃ§Ã£o Local](#instalaÃ§Ã£o-local)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [Uso](#uso)
  - [Exemplos PrÃ¡ticos](#exemplos-prÃ¡ticos)
- [Arquitetura](#arquitetura)
- [Estrutura dos Dados](#estrutura-dos-dados)
- [Monitoramento e Logs](#monitoramento-e-logs)
- [Testes](#testes)
- [Troubleshooting](#troubleshooting)
- [Boas PrÃ¡ticas](#boas-prÃ¡ticas)

## âœ¨ CaracterÃ­sticas

### Funcionalidades Principais

- âœ… **Busca por NÃºmero de Processo**: Suporta um ou mÃºltiplos processos (separados por vÃ­rgula)
- âœ… **Busca por CNPJ**: Extrai automaticamente todos os processos relacionados a um CNPJ
- âœ… **ExtraÃ§Ã£o Completa de Dados**:
  - InformaÃ§Ãµes bÃ¡sicas do processo (nÃºmero, nÃºmero legado, data de autuaÃ§Ã£o)
  - Partes envolvidas (advogados, procuradores, apelantes, etc.)
  - HistÃ³rico completo de movimentaÃ§Ãµes com datas
- âœ… **PersistÃªncia MongoDB**: 
  - Pipeline otimizado com operaÃ§Ã£o upsert (insert ou update automÃ¡tico)
  - Ãndice Ãºnico por nÃºmero de processo
  - Timestamps de criaÃ§Ã£o e atualizaÃ§Ã£o
- âœ… **ExportaÃ§Ã£o JSON**: Salva dados localmente em formato JSON
- âœ… **Monitoramento AvanÃ§ado**:
  - Middleware de tracking de tempo de resposta
  - Log detalhado de erros com salvamento de HTML para debug
  - EstatÃ­sticas de execuÃ§Ã£o (itens processados, tempo decorrido, etc.)
- âœ… **ResiliÃªncia e Confiabilidade**:
  - AutoThrottle para ajuste automÃ¡tico de velocidade
  - Sistema de retry configurÃ¡vel para requisiÃ§Ãµes falhadas
  - ValidaÃ§Ã£o de nÃºmeros de processo
  - Tratamento robusto de erros
- âœ… **Ambiente Dockerizado**: Stack completa com MongoDB e spider prÃ©-configurados

## ğŸ›  Tecnologias Utilizadas

- **Python 3.10+**: Linguagem principal
- **Scrapy 2.11+**: Framework de web scraping
- **MongoDB 8.2.1**: Banco de dados NoSQL para armazenamento
- **PyMongo 4.6+**: Driver Python para MongoDB
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o e orquestraÃ§Ã£o
- **ItemLoaders**: Processamento e limpeza de dados
- **python-dateutil**: Parsing de datas

## ğŸ”§ Requisitos

### Sistema Operacional
- Windows 10/11, Linux ou macOS

### Software NecessÃ¡rio

#### Para execuÃ§Ã£o com Docker (Recomendado)
- **Docker Engine**: 20.10 ou superior
- **Docker Compose**: 2.0 ou superior

#### Para execuÃ§Ã£o local
- **Python**: 3.8 ou superior (recomendado 3.10+)
- **MongoDB**: 4.0 ou superior (ou usar MongoDB via Docker)
- **pip**: Gerenciador de pacotes Python

### DependÃªncias Python

Todas as dependÃªncias estÃ£o listadas em `requirements.txt`:
```
scrapy>=2.11.0
pymongo>=4.6.0
w3lib>=2.1.2
python-dateutil>=2.8.2
itemloaders>=1.1.0
```

## ğŸ“¦ InstalaÃ§Ã£o

### InstalaÃ§Ã£o com Docker (Recomendado)

A forma mais rÃ¡pida e fÃ¡cil de executar o projeto! O Docker gerencia automaticamente o MongoDB e todas as dependÃªncias.

#### OpÃ§Ã£o 1: InÃ­cio RÃ¡pido

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd teste_pratico_scraping

# Inicie toda a stack (MongoDB + Spider)
docker-compose up
```

Isso irÃ¡:
1. Baixar as imagens necessÃ¡rias (MongoDB 8.2.1 e Python 3.10)
2. Criar os containers
3. Configurar o MongoDB com autenticaÃ§Ã£o
4. Aguardar o MongoDB ficar saudÃ¡vel
5. Deixar o spider pronto para execuÃ§Ã£o

#### OpÃ§Ã£o 2: Usando o Script de Quick Start

```bash
# Linux/Mac
chmod +x docker-quickstart.sh
./docker-quickstart.sh

# Windows (PowerShell)
.\docker-quickstart.sh
```

#### Executar o Spider com Docker

```bash
# Buscar por nÃºmero de processo
docker-compose run --rm spider scrapy crawl processo -a processos="00156487819994050000"

# Buscar por CNPJ
docker-compose run --rm spider scrapy crawl processo -a cnpj="12.345.678/0001-90"

# MÃºltiplos processos
docker-compose run --rm spider scrapy crawl processo -a processos="00156487819994050000,00234567890123456789"
```

**ğŸ“– Consulte a [DocumentaÃ§Ã£o completa do Docker](DOCKER.md) para mais detalhes e comandos avanÃ§ados.**

### InstalaÃ§Ã£o Local

Para desenvolvedores que preferem executar diretamente no sistema operacional.

#### 1. Clone o RepositÃ³rio

```bash
git clone <repository-url>
cd teste_pratico_scraping
```

#### 2. Crie e Ative o Ambiente Virtual

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar no Windows (PowerShell)
.\venv\Scripts\Activate.ps1

# Ativar no Windows (CMD)
.\venv\Scripts\activate.bat

# Ativar no Linux/Mac
source venv/bin/activate
```

#### 3. Instale as DependÃªncias

```bash
pip install -r requirements.txt
```

#### 4. Configure o MongoDB

**OpÃ§Ã£o A: MongoDB via Docker (Recomendado para desenvolvimento local)**

```bash
docker run -d \
  --name mongodb \
  -p 27017:27017 \
  -e MONGO_INITDB_ROOT_USERNAME=admin \
  -e MONGO_INITDB_ROOT_PASSWORD=1234 \
  mongo:8.2.1

# Windows PowerShell
docker run -d `
  --name mongodb `
  -p 27017:27017 `
  -e MONGO_INITDB_ROOT_USERNAME=admin `
  -e MONGO_INITDB_ROOT_PASSWORD=1234 `
  mongo:8.2.1
```

**OpÃ§Ã£o B: InstalaÃ§Ã£o Local do MongoDB**

1. Baixe o MongoDB Community Server: https://www.mongodb.com/try/download/community
2. Siga as instruÃ§Ãµes de instalaÃ§Ã£o para seu sistema operacional
3. Inicie o serviÃ§o MongoDB

#### 5. Verifique a InstalaÃ§Ã£o

```bash
# Teste a conexÃ£o com MongoDB
python verify_mongodb.py

# Liste os spiders disponÃ­veis
scrapy list

# Deve mostrar: processo
```

## âš™ï¸ ConfiguraÃ§Ã£o

### ConfiguraÃ§Ãµes Principais

As configuraÃ§Ãµes do projeto estÃ£o em `trf_scraper/settings.py`.

#### MongoDB

```python
# ConexÃ£o com MongoDB
MONGO_URI = "mongodb://admin:1234@localhost:27017/"
MONGO_DATABASE = "trf5_processos"

# Para MongoDB sem autenticaÃ§Ã£o
MONGO_URI = "mongodb://localhost:27017/"
```

#### Rate Limiting e Performance

```python
# RequisiÃ§Ãµes simultÃ¢neas
CONCURRENT_REQUESTS = 8

# Delay entre requisiÃ§Ãµes (em segundos)
DOWNLOAD_DELAY = 1

# AutoThrottle - ajuste automÃ¡tico de velocidade
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10

# Tentativas de retry em caso de erro
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]
```

#### Middlewares

```python
DOWNLOADER_MIDDLEWARES = {
    'trf_scraper.middlewares.ResponseTimeMiddleware': 543,
    'trf_scraper.middlewares.ErrorLoggingMiddleware': 544,
}
```

#### Pipelines

```python
ITEM_PIPELINES = {
    'trf_scraper.pipelines.MongoDBPipeline': 300,
}
```

### VariÃ¡veis de Ambiente (Docker)

Ao usar Docker, vocÃª pode sobrescrever configuraÃ§Ãµes via variÃ¡veis de ambiente:

```bash
docker-compose run --rm \
  -e MONGO_URI="mongodb://user:pass@host:27017/" \
  -e MONGO_DATABASE="outro_banco" \
  spider scrapy crawl processo -a processos="123"
```

## ğŸš€ Uso

### Comandos BÃ¡sicos

#### Busca por NÃºmero de Processo

```bash
# Executar localmente
scrapy crawl processo -a processos="00156487819994050000"

# Executar com Docker
docker-compose run --rm spider scrapy crawl processo -a processos="00156487819994050000"
```

#### Busca por CNPJ

```bash
# Executar localmente
scrapy crawl processo -a cnpj="12.345.678/0001-90"

# Executar com Docker
docker-compose run --rm spider scrapy crawl processo -a cnpj="12345678000190"
```

> **Nota**: O CNPJ pode ser informado com ou sem formataÃ§Ã£o (pontos, barras e traÃ§os).

#### MÃºltiplos Processos

```bash
# Separar processos por vÃ­rgula
scrapy crawl processo -a processos="00156487819994050000,00234567890123456789,00987654321098765432"
```

#### Busca Combinada

```bash
# Processos especÃ­ficos + todos os processos de um CNPJ
scrapy crawl processo \
  -a processos="00156487819994050000,00234567890123456789" \
  -a cnpj="12.345.678/0001-90"
```

#### Exportar para JSON

```bash
# Salva no MongoDB E em arquivo JSON
scrapy crawl processo -a processos="00156487819994050000" -o output.json

# JSON formatado (pretty print)
scrapy crawl processo -a processos="00156487819994050000" -o output.json:json -s FEED_EXPORT_INDENT=2
```

### NÃ­veis de Log

```bash
# INFO - Apenas informaÃ§Ãµes importantes (padrÃ£o)
scrapy crawl processo -a processos="..." -L INFO

# DEBUG - MÃ¡ximo detalhe para debugging
scrapy crawl processo -a processos="..." -L DEBUG

# WARNING - Apenas avisos e erros
scrapy crawl processo -a processos="..." -L WARNING

# ERROR - Apenas erros
scrapy crawl processo -a processos="..." -L ERROR
```

### Salvar Logs em Arquivo

```bash
# Executar localmente com log em arquivo
scrapy crawl processo -a processos="..." --logfile=logs/scraping.log

# Com Docker (logs salvos em ./logs/)
docker-compose run --rm spider \
  scrapy crawl processo -a processos="..." --logfile=/app/logs/scraping.log
```

### Exemplos PrÃ¡ticos

#### Exemplo 1: Extrair um Ãºnico processo

```bash
scrapy crawl processo -a processos="00156487819994050000" -L INFO
```

**SaÃ­da esperada**:
```
[processo] INFO: Spider inicializado - Processos: 1, CNPJ: False
[processo] INFO: Criando request para processo: 00156487819994050000
[processo] INFO: Processando pÃ¡gina do processo: https://cp.trf5.jus.br/cp/cp.do
[processo] INFO: Processo inserido: PROCESSO NÂº 0015648-78.1999.4.05.0000
[scrapy.core.engine] INFO: Spider closed (finished)
```

#### Exemplo 2: Extrair todos os processos de um CNPJ

```bash
scrapy crawl processo -a cnpj="12.345.678/0001-90" -o processos_empresa.json
```

#### Exemplo 3: Busca combinada com log detalhado

```bash
scrapy crawl processo \
  -a processos="00156487819994050000" \
  -a cnpj="12.345.678/0001-90" \
  -L DEBUG \
  --logfile=logs/busca_completa.log
```

#### Exemplo 4: Verificar dados no MongoDB

```bash
# Use o script Python fornecido
python verify_mongodb.py

# Ou conecte diretamente com mongosh
mongosh "mongodb://admin:1234@localhost:27017/" --authenticationDatabase admin
```

No mongosh:
```javascript
use trf5_processos

// Contar total de processos
db.processos.countDocuments()

// Buscar um processo especÃ­fico
db.processos.findOne({numero_processo: /0015648/})

// Listar Ãºltimos 5 processos inseridos
db.processos.find().sort({created_at: -1}).limit(5)

// Buscar processos por data de extraÃ§Ã£o
db.processos.find({data_extracao: {$gte: "2025-11-01"}})
```

## ğŸ—ï¸ Arquitetura

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Spider    â”‚ Inicia requisiÃ§Ãµes HTTP
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResponseTimeMiddleware  â”‚ Monitora tempo de resposta
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRF5 Site  â”‚ Responde com HTML
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Parser    â”‚ Extrai dados com XPath/CSS
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Items     â”‚ Estrutura e valida dados
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MongoDBPipeline     â”‚ Salva no banco (upsert)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MongoDB    â”‚ Armazenamento persistente
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estrutura do Projeto

```
teste_pratico_scraping/
â”œâ”€â”€ trf_scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py              # DefiniÃ§Ã£o dos Items (ProcessoItem, EnvolvidoItem, MovimentacaoItem)
â”‚   â”œâ”€â”€ middlewares.py        # Middlewares customizados
â”‚   â”‚                         # - ResponseTimeMiddleware: monitora tempo de resposta
â”‚   â”‚                         # - ErrorLoggingMiddleware: registra erros
â”‚   â”œâ”€â”€ pipelines.py          # MongoDBPipeline com lÃ³gica de upsert
â”‚   â”œâ”€â”€ settings.py           # ConfiguraÃ§Ãµes do Scrapy
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ processo_spider.py # Spider principal
â”‚
â”œâ”€â”€ logs/                     # DiretÃ³rio para logs de execuÃ§Ã£o
â”œâ”€â”€ output/                   # Arquivos JSON e HTML de debug
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ Dockerfile                # Imagem do spider
â”œâ”€â”€ docker-entrypoint.sh      # Script de inicializaÃ§Ã£o do container
â”œâ”€â”€ docker-quickstart.sh      # Script de quick start
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ scrapy.cfg                # ConfiguraÃ§Ã£o do projeto Scrapy
â”œâ”€â”€ verify_mongodb.py         # Script para verificar dados no MongoDB
â”œâ”€â”€ DOCKER.md                 # DocumentaÃ§Ã£o completa do Docker
â””â”€â”€ README.md                 # Este arquivo
```

### Componentes Principais

#### 1. Spider (`processo_spider.py`)

ResponsÃ¡vel por:
- Navegar pelo site do TRF5
- Fazer requisiÃ§Ãµes HTTP (GET e POST)
- Extrair dados das pÃ¡ginas HTML
- Gerenciar busca por processo e por CNPJ
- Validar nÃºmeros de processo
- Salvar HTML de debug em caso de erro

**Principais mÃ©todos**:
- `start_requests()`: Inicia o processo de scraping
- `parse_form_cnpj()`: Processa o formulÃ¡rio para busca por CNPJ
- `parse_processo()`: Extrai dados de um processo
- `parse_lista_processos()`: Processa lista de processos (busca por CNPJ)
- `_extract_envolvidos()`: Extrai partes envolvidas
- `_extract_movimentacoes()`: Extrai histÃ³rico de movimentaÃ§Ãµes

#### 2. Items (`items.py`)

Define a estrutura dos dados extraÃ­dos:

```python
ProcessoItem:
    - numero_processo: str
    - numero_legado: str
    - data_autuacao: str
    - url: str
    - data_extracao: datetime
    - envolvidos: List[EnvolvidoItem]
    - movimentacoes: List[MovimentacaoItem]

EnvolvidoItem:
    - papel: str  # Ex: "APTE", "Advogado/Procurador"
    - nome: str

MovimentacaoItem:
    - data: str
    - texto: str
```

#### 3. Middlewares (`middlewares.py`)

- **ResponseTimeMiddleware**: Monitora e loga o tempo de resposta de cada requisiÃ§Ã£o
- **ErrorLoggingMiddleware**: Captura e registra URLs com falha para retry posterior

#### 4. Pipeline (`pipelines.py`)

**MongoDBPipeline** implementa:
- ConexÃ£o com MongoDB com autenticaÃ§Ã£o
- OperaÃ§Ã£o **upsert** (insert ou update) baseada em `numero_processo`
- CriaÃ§Ã£o automÃ¡tica de Ã­ndice Ãºnico
- Timestamps automÃ¡ticos (`created_at`, `updated_at`)
- SerializaÃ§Ã£o de datetime e nested items
- Tratamento robusto de erros
- EstatÃ­sticas de operaÃ§Ãµes (inseridos, atualizados, erros)

### Fluxo de ExecuÃ§Ã£o Detalhado

1. **InicializaÃ§Ã£o**
   - Spider recebe parÃ¢metros (processos e/ou cnpj)
   - Valida que pelo menos um parÃ¢metro foi fornecido
   - Loga informaÃ§Ãµes de inicializaÃ§Ã£o

2. **RequisiÃ§Ãµes**
   - **Para processos individuais**: Acessa diretamente via GET em `https://cp.trf5.jus.br/processo/{numero}` (mais rÃ¡pido)
   - **Para CNPJ**: Acessa pÃ¡gina inicial do TRF5 e cria FormRequest para buscar lista de processos

3. **Parsing**
   - Extrai dados usando seletores XPath e CSS
   - Usa ItemLoader para processar e limpar dados
   - Valida nÃºmeros de processo (deve ter 20 dÃ­gitos)
   - Extrai envolvidos e movimentaÃ§Ãµes como sub-items

4. **Pipeline**
   - Recebe item processado
   - Serializa datetime e nested objects
   - Executa upsert no MongoDB
   - Atualiza estatÃ­sticas

5. **FinalizaÃ§Ã£o**
   - Fecha conexÃ£o com MongoDB
   - Exibe estatÃ­sticas finais
   - Salva arquivos JSON (se configurado)

## ï¿½ Estrutura dos Dados

### Documento MongoDB

Cada processo Ã© armazenado como um documento no MongoDB com a seguinte estrutura:

```json
{
  "_id": ObjectId("..."),
  "numero_processo": "0015648-78.1999.4.05.0000",
  "numero_legado": "(99.05.15648-8)",
  "data_autuacao": "15/04/1999",
  "url": "https://cp.trf5.jus.br/processo/00156487819994050000",
  "data_extracao": "2025-11-11 14:35:54",
  "envolvidos": [
    {
      "papel": "APTE",
      "nome": "MARIA MARLENE GOMES MARQUES(e outros)"
    },
    {
      "papel": "Advogado/Procurador",
      "nome": "JALES DE SENA RIBEIRO - CE006397"
    }
  ],
  "movimentacoes": [
    {
      "data": "15/04/1999 00:00:00",
      "texto": "Processo distribuÃ­do."
    },
    {
      "data": "11/09/2021 16:50:00",
      "texto": "Baixa Definitiva - Processo Migrado para o PJe ."
    }
  ],
  "created_at": ISODate("2025-11-11T14:35:54.123Z"),
  "updated_at": ISODate("2025-11-11T14:35:54.123Z")
}
```

### DescriÃ§Ã£o dos Campos

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| `numero_processo` | String | NÃºmero completo do processo (limpo, sem prefixos) |
| `numero_legado` | String | NÃºmero no formato antigo |
| `data_autuacao` | String | Data de autuaÃ§Ã£o do processo (formato: DD/MM/YYYY ou datetime) |
| `url` | String | URL da pÃ¡gina do processo |
| `data_extracao` | String | Data/hora da extraÃ§Ã£o |
| `envolvidos` | Array | Partes envolvidas |
| `envolvidos[].papel` | String | Papel da parte (APTE, APDO, Advogado/Procurador, RELATOR, etc.) |
| `envolvidos[].nome` | String | Nome da parte |
| `movimentacoes` | Array | HistÃ³rico de movimentaÃ§Ãµes |
| `movimentacoes[].data` | String | Data da movimentaÃ§Ã£o (formato: DD/MM/YYYY HH:MM:SS ou datetime) |
| `movimentacoes[].texto` | String | DescriÃ§Ã£o da movimentaÃ§Ã£o |
| `created_at` | ISODate | Data de criaÃ§Ã£o no MongoDB |
| `updated_at` | ISODate | Data da Ãºltima atualizaÃ§Ã£o |

### Ãndices MongoDB

Ãndice Ãºnico criado automaticamente:

```javascript
db.processos.createIndex({ "numero_processo": 1 }, { unique: true })
```

## ğŸ“ˆ Monitoramento e Logs

### EstatÃ­sticas de ExecuÃ§Ã£o

```
2025-11-11 14:35:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{
 'downloader/request_count': 15,
 'item_scraped_count': 10,
 'mongodb/items_inserted': 8,
 'mongodb/items_updated': 2,
 'elapsed_time_seconds': 45.3
}
```

### Logs do Pipeline

```
[processo] INFO: ConexÃ£o com MongoDB estabelecida com sucesso.
[processo] INFO: Processo inserido: PROCESSO NÂº 0015648-78.1999.4.05.0000
[processo] INFO: Processo atualizado: PROCESSO NÂº 0015648-78.1999.4.05.0000
```

### Debug HTML

Salvamento automÃ¡tico de HTML para anÃ¡lise de erros em `output/`:
- `erro_processo_*.html`: PÃ¡ginas de erro
- `lista_cnpj_vazia_*.html`: Buscas sem resultados
- `processo_sem_numero_*.html`: Processos sem nÃºmero

## ğŸ§ª Testes

O projeto possui uma suÃ­te completa de testes automatizados cobrindo todas as funcionalidades principais.

### InstalaÃ§Ã£o das DependÃªncias de Teste

```bash
# Ative o ambiente virtual
# Windows
.\venv\Scripts\Activate.ps1

# Linux/Mac
source venv/bin/activate

# Instale dependÃªncias de teste
pip install -r requirements-test.txt
```

### Executar os Testes

```bash
# Executar todos os testes (unittest)
python -m unittest discover tests -v

# Executar todos os testes (pytest - recomendado)
pytest

# Executar com cobertura de cÃ³digo
pytest --cov=trf_scraper --cov-report=html

# Executar apenas um arquivo de testes
pytest tests/test_items.py -v

# Executar teste especÃ­fico
pytest tests/test_items.py::TestCleanFunctions::test_clean_numero_processo_with_prefix
```

### Estrutura dos Testes

```
tests/
â”œâ”€â”€ test_items.py          # Testes dos Items e processadores (17 testes)
â”œâ”€â”€ test_spider.py         # Testes do spider (15 testes)
â”œâ”€â”€ test_pipelines.py      # Testes do pipeline MongoDB (8 testes)
â”œâ”€â”€ test_middlewares.py    # Testes dos middlewares (14 testes)
â””â”€â”€ test_integration.py    # Testes de integraÃ§Ã£o (3 testes)
```

### Cobertura de CÃ³digo

```bash
# Gerar relatÃ³rio de cobertura
pytest --cov=trf_scraper --cov-report=html

# Abrir relatÃ³rio no navegador
# Windows
start htmlcov/index.html

# Linux/Mac
open htmlcov/index.html
```

### Testes com Docker

```bash
# Executar testes dentro do container
docker-compose run --rm spider python -m pytest

# Com cobertura
docker-compose run --rm spider pytest --cov=trf_scraper
```

**ï¿½ Consulte a [DocumentaÃ§Ã£o completa de Testes](TESTING.md) para mais detalhes.**

## ï¿½ğŸ“ Exemplos

### Verificar Dados no MongoDB

```python
from pymongo import MongoClient

client = MongoClient("mongodb://admin:1234@localhost:27017/")
db = client['trf5_processos']

# Contar processos
print(f"Total: {db.processos.count_documents({})}")

# Buscar um processo
processo = db.processos.find_one({"numero_processo": {"$regex": "0015648"}})
print(processo)
```

Ou use o script pronto:

```bash
python verify_mongodb.py
```

### Exportar para JSON

```bash
# Durante a execuÃ§Ã£o do scraping
scrapy crawl processo -a processos="..." -o output.json

# Exportar do MongoDB para JSON usando mongoexport
mongoexport \
  --uri="mongodb://admin:1234@localhost:27017/" \
  --db=trf5_processos \
  --collection=processos \
  --out=processos_export.json \
  --pretty

# Windows PowerShell
mongoexport `
  --uri="mongodb://admin:1234@localhost:27017/" `
  --db=trf5_processos `
  --collection=processos `
  --out=processos_export.json `
  --pretty
```

### Backup e Restore

```bash
# Backup completo do banco
mongodump \
  --uri="mongodb://admin:1234@localhost:27017/" \
  --db=trf5_processos \
  --out=backup_$(date +%Y%m%d)

# Restore do backup
mongorestore \
  --uri="mongodb://admin:1234@localhost:27017/" \
  --db=trf5_processos \
  backup_20251111/trf5_processos
```
